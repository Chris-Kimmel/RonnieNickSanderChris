{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This script downloads all the files listed in \"data_files_to_download.txt\", extracts them, and places them in the \"data\" folder. It takes two minutes to run on my laptop.**\n",
    "\n",
    "The \"data_files_to_download.txt\" file right now (5/19) only contains links to the datasets listed on this webpage: https://botometer.iuni.iu.edu/bot-repository/datasets.html. (The same link is in our Slack channel.)\n",
    "\n",
    "You don't have to worry about deleting the old data files before re-running the notebook; they will be overwritten.\n",
    "\n",
    "The \"data\" folder is listed in our git repository's \"gitignore\" file, which means nothing inside it will get pushed to the online github repository. This is because our data files are, together, hundreds of megabytes and we don't have enough room on our online repository to hold all that data.\n",
    "\n",
    "The output includes some folders and zipped csv files; I don't know where they come from. Probably one of caverlee-2011, cresci-2017, or cresci-2015.\n",
    "\n",
    "Hopefully this helps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle files and directories\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# handle internet\n",
    "import requests\n",
    "\n",
    "# decompress files of various formats\n",
    "import tarfile\n",
    "import zipfile\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a \"data\" Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is already a data directory\n"
     ]
    }
   ],
   "source": [
    "if 'data' not in os.listdir():\n",
    "    os.mkdir('data')\n",
    "    print('Created data directory')\n",
    "else:\n",
    "    assert os.path.isdir('data'), \"There is a file named 'data', but the program needs a directory named 'data'\"\n",
    "    print('There is already a data directory')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dload_extract(url, dirpath):\n",
    "    \n",
    "    # Move into directory\n",
    "    os.chdir(dirpath)\n",
    "    \n",
    "    # Download\n",
    "    print('downloading...', end=' ')\n",
    "    r = requests.get(url)\n",
    "    compressed_filename = url.split('/')[-1]\n",
    "    compressed_file_path = compressed_filename\n",
    "    compressed_file = open(compressed_filename, 'wb')\n",
    "    compressed_file.write(r.content)\n",
    "    compressed_file.close()\n",
    "    print('complete')\n",
    "    \n",
    "    # Extract\n",
    "    print('extracting...', end=' ')\n",
    "    if '.tar' in compressed_filename:\n",
    "        tar = tarfile.open(compressed_filename, 'r:*')\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "    elif compressed_filename.endswith('.zip'):\n",
    "        with zipfile.ZipFile(compressed_filename,\"r\") as zip_ref:\n",
    "            zip_ref.extractall()\n",
    "            shutil.rmtree('__MACOSX', ignore_errors=True)\n",
    "    elif compressed_filename.endswith('.gz'):\n",
    "        print('gzip file')\n",
    "    print('complete')\n",
    "    \n",
    "    print('cleaning up...', end=' ')\n",
    "    # Remove compressed file\n",
    "    os.remove(compressed_filename)\n",
    "    \n",
    "    # Return to original directory\n",
    "    os.chdir('..')\n",
    "    print('complete')\n",
    "    \n",
    "def return_to_project_dir():\n",
    "    current_dir_name = os.path.basename(os.getcwd())\n",
    "    assert current_dir_name in {'RonnieNickSanderChris', 'data'}, \\\n",
    "        'Program lost track of which folder it\\'s in. Try restarting the notebook kernel.'\n",
    "    if current_dir_name == 'data':\n",
    "        os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_to_dload_and_extract(url, dirpath):\n",
    "    return_to_project_dir()\n",
    "    print('extracting', url.split('/')[-1])\n",
    "    print('source:', format(url))\n",
    "    try:\n",
    "        dload_extract(url, dirpath)\n",
    "        print('extraction successful')\n",
    "        print()\n",
    "    except tarfile.ReadError as e:\n",
    "        print('extraction failed:', e)\n",
    "        return_to_project_dir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and Extract Everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_to_project_dir()\n",
    "with open('data_files_to_download.txt', 'rt') as file:\n",
    "    lines_including_comments = file.read().splitlines()\n",
    "    # We ignore lines starting with the pound symbol in data_files_to_download.txt\n",
    "    data_files_to_download = [line for line in lines_including_comments if line[0] != '#']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting verified-2019.tar.gz\n",
      "source: https://botometer.iuni.iu.edu/bot-repository/datasets/verified-2019/verified-2019.tar.gz\n",
      "extraction successful\n",
      "\n",
      "extracting botwiki-2019.tar.gz\n",
      "source: https://botometer.iuni.iu.edu/bot-repository/datasets/botwiki-2019/botwiki-2019.tar.gz\n",
      "extraction successful\n",
      "\n",
      "extracting cresci-rtbust-2019.tar.gz\n",
      "source: https://botometer.iuni.iu.edu/bot-repository/datasets/cresci-rtbust-2019/cresci-rtbust-2019.tar.gz\n",
      "extraction successful\n",
      "\n",
      "extracting political-bots-2019.tar.gz\n",
      "source: https://botometer.iuni.iu.edu/bot-repository/datasets/political-bots-2019/political-bots-2019.tar.gz\n",
      "extraction successful\n",
      "\n",
      "extracting botometer-feedback-2019.tar.gz\n",
      "source: https://botometer.iuni.iu.edu/bot-repository/datasets/botometer-feedback-2019/botometer-feedback-2019.tar.gz\n",
      "extraction successful\n",
      "\n",
      "extracting vendor-purchased-2019.tar.gz\n",
      "source: https://botometer.iuni.iu.edu/bot-repository/datasets/vendor-purchased-2019/vendor-purchased-2019.tar.gz\n",
      "extraction successful\n",
      "\n",
      "extracting celebrity-2019.tar.gz\n",
      "source: https://botometer.iuni.iu.edu/bot-repository/datasets/celebrity-2019/celebrity-2019.tar.gz\n",
      "extraction successful\n",
      "\n",
      "extracting pronbots-2019.tar.gz\n",
      "source: https://botometer.iuni.iu.edu/bot-repository/datasets/pronbots-2019/pronbots-2019.tar.gz\n",
      "extraction successful\n",
      "\n",
      "extracting midterm-2018.tar.gz\n",
      "source: https://botometer.iuni.iu.edu/bot-repository/datasets/midterm-2018/midterm-2018.tar.gz\n",
      "extraction successful\n",
      "\n",
      "extracting cresci-stock-2018.tar.gz\n",
      "source: https://botometer.iuni.iu.edu/bot-repository/datasets/cresci-stock-2018/cresci-stock-2018.tar.gz\n",
      "extraction successful\n",
      "\n",
      "extracting gilani-2017.tar.gz\n",
      "source: https://botometer.iuni.iu.edu/bot-repository/datasets/gilani-2017/gilani-2017.tar.gz\n",
      "extraction successful\n",
      "\n",
      "extracting varol-2017.dat.gz\n",
      "source: https://botometer.iuni.iu.edu/bot-repository/datasets/varol-2017/varol-2017.dat.gz\n",
      "gzip file\n",
      "extraction successful\n",
      "\n",
      "extracting caverlee-2011.zip\n",
      "source: https://botometer.iuni.iu.edu/bot-repository/datasets/caverlee-2011/caverlee-2011.zip\n",
      "extraction successful\n",
      "\n",
      "extracting cresci-2017.csv.zip\n",
      "source: https://botometer.iuni.iu.edu/bot-repository/datasets/cresci-2017/cresci-2017.csv.zip\n",
      "extraction successful\n",
      "\n",
      "extracting cresci-2015.csv.tar.gz\n",
      "source: https://botometer.iuni.iu.edu/bot-repository/datasets/cresci-2015/cresci-2015.csv.tar.gz\n",
      "extraction successful\n",
      "\n",
      "\n",
      "DONE!\n"
     ]
    }
   ],
   "source": [
    "for url in data_files_to_download:\n",
    "    try_to_dload_and_extract(url, 'data')\n",
    "\n",
    "print()\n",
    "print('DONE!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
